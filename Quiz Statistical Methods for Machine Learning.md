# Quiz list for the written test

1) **Write the formula for the square loss, the zero-one loss, and the logarithmic loss.**
	The square loss formula is $\ell(y, \widehat{y}) = (y - \widehat{y})^2$;
	the zero-one loss is $\ell(y, \widehat{y}) = \cases{1 \quad \text{ if } y \neq \widehat{y} \cr \cr 0 \quad \text{ otherwise }}$
	the logarithmic loss is $\ell(y, \widehat{y}) = \cases{\ln{\frac{1}{\widehat{y}}} \quad \text{ if } y = 1 \cr \cr \ln{\frac{1}{1 - \widehat{y}}} \quad \text{ if } y = 0}$
	where $y$ is the label of the data point and $\widehat{y}$ is the predicted label.
2) **Write the mathematical formula defining the training error of a predictor $h$.**
	The training error or empirical risk is $\ell_s(h) = \frac{1}{m}\sum_{t=1}^{m}\ell(y_t, h(\mathbf{x}_t))$, where $S$ is the training set of size $m$, $\ell$ is a loss function, $h: \mathcal{X} \to \mathcal{Y}$ is a predictor and $(x_t, y_t)$ is a training example from the training set.
3) **What does a learning algorithm receive in input? And what does it produce in output?**
	A learning algorithm receives a training set in input and it produces a predictor in output. A training set $S$ is a set of examples $\{(x_1, y_1), ..., (x_m, y_m)\}$, where $x$ is a data point and $y$ is the corresponding label. Given the set $\mathcal{X}$ of all possible data points for a given learning problem and given the set $\mathcal{Y}$ of all possible labels, a predictor is a function $f: \mathcal{X} \to \mathcal{Y}$ mapping data points to labels (or $f: \mathcal{X} \to \mathcal{Z}$ if the predictions belong to a set $\mathcal{Z}$ different from $\mathcal{Y}$).
4) **Write the mathematical formula defining the ERM algorithm over a class $\mathcal{H}$ of predictors. Define the main quantities occurring in the formula.**
	Given a class $\mathcal{H}$ of predictors and a loss function $\ell$, the Empirical Risk Minimizer is the learning algorithm that outputs some predictors minimizing the empirical risk on the training set $S$. The mathematical formula is $\widehat{h} \in \underset{h \in \mathcal{H}}{\operatorname{argmin}} \ell_S(h)$. The $\widehat{h} \in$ notation takes into account the fact that there could be multiple $h \in \mathcal{H}$ minimizing the training error.
5) **How do you define overfitting and underfitting in terms of behavior of an algorithm on training and test set?**
	it is possible to give specific names to the two ways of failing for a generic learning algorithm $A$, that is, when it returns a predictor with high test error:
	- if $A$ fails by returning predictors with high training error, then we say that $A$ is **underfitting**;
	- if $A$ fails by returning predictors with low training error, then we say that $A$ is **overfitting**.
6) **Name and describe three reasons why labels may be noisy.**
	Noise may occur for at least three (not mutually exclusive) reasons:
	1) **human in the loop**: the labels are assigned by a human annotator who decides the “true” label for each data point. In this case, different annotators may have different opinions;
	2) **epistemic uncertainty**: each data point is represented by a feature vector $x$ that does not contain enough information to uniquely determine the label;
	3) **aleatoric uncertainty**: the feature vector $x$ representing a data point is obtained through noisy measurements. The label associated with a given $x$ is then stochastic because the same $x$ could have been generated by different data points.
	
	Noisy labels cause overfitting because they may mislead the algorithm with regard to what is the “true” label for a given data point.
7) **Write a short pseudo-code for the $k-NN$ algorithm for binary classification. (BONUS)**
	In order to compute $h_{k−NN}(x)$, the following operations are performed:
	1) find the $k$ training points $x_{t_1} ,..., x_{t_k}$ closest to $x$ (if there are more than $k$ training points closest to $x$, choose the $k'$ training points where $k'$ is the smallest integer bigger or equal to $k$ such that the $(k' +1)$-th data point has distance from $x$ strictly larger than the $k'$-th point). Let $y_{t_1} ,...,y_{t_k}$ be their labels;
	2) if the majority of the labels $y_{t_1} ,...,y_{t_k}$ is $+1$, then $h_{k−NN}(x) = +1$; if the majority is $−1$, then $h_{k−NN}(x) = −1$. If there is an equal number of closest points with positive and negative labels, then the algorithm predicts a default value in $\{−1,1\}$ (for instance, the most frequent label in the training set).
8) **Is $k-NN$ more likely to overfit when $k$ is large or small?**
	The learning algorithm suffers from high test error for small values of $k$ (overfitting) and for large values of $k$ (underfitting).
9) **Write a short pseudo-code for building a tree classifier based on a training set $S$.**
	1) **initialization**: create a tree $T$ with only the root $\ell$ and route each data point to the root, that is, $S_\ell = S$. Let the label associated with the root be the most frequent label in $S_\ell$;
	2) **main loop**: pick a leaf $\ell$ and replace it with an internal node $v$ creating two children $\ell'$ (first child) and $\ell''$ (second child). Pick an attribute $i$ and a test $f : \mathcal{X}_i \to \{1,2\}$. Associate the test $f$ with $v$ and partition $S_\ell$ in the two subsets $S_{\ell '} = \{(x_t, y_t) \in S_{\ell} : f(x_{t, i}) = 1\}$ and $S_{\ell''} = \{(x_t, y_t) \in S_{\ell} : f(x_{t, i}) = 2\}$. Let the labels associated with $\ell'$ and $\ell''$ be, respectively, the most frequent labels in $S_{\ell'}$ and $S_{\ell''}$;
10) **What is the property of a splitting criterion $\psi$ ensuring that the training error of a tree classifier does not increase after a split? Bonus points if you justify your answer with a proof.**
	The property of a splitting criterion $\psi$ ensuring that the training error of a tree classifier does not increase after a split is the fact that $\psi$ is a concave function.
	To prove it, we can utilize Jensen's inequality, stating that $\psi(\alpha a + (1 -\alpha)b) \geq \alpha \psi (a) + (1 - \alpha) \psi (b) \quad \forall a, b \in \mathbb{R} \text{ and all } \alpha \in [0,1]$.<br />
	We can study how the training error changes when $\ell$ is replaced by two new leaves $\ell'$ and $\ell''$, $\underbrace{\psi \Big ( \frac{N^+_\ell}{N_\ell} \Big)N_\ell}_{\text{contribution of }\ell \text { to the training error}} = \psi \Big ( \frac{N^+_{\ell'}}{N_\ell} + \frac{N^+_{\ell''}}{N_\ell} \Big)N_\ell = \psi \Big ( \frac{N^+_{\ell'}}{N_\ell} \frac{N_{\ell'}}{N_{\ell'}} + \frac{N^+_{\ell''}}{N_\ell}\frac{N_{\ell''}}{N_{\ell''}} \Big)N_\ell =$
	At this point it is possible to apply Jensen's inequality: $= \psi \Big ( \text{ }\underbrace{\frac{N^+_{\ell'}}{N_{\ell'}}}_{a} \underbrace{\frac{N_{\ell'}}{N_\ell}}_{\alpha} + \underbrace{\frac{N^+_{\ell''}}{N_{\ell''}}}_{b} \underbrace{\frac{N_{\ell''}}{N_{\ell}}}_{1- \alpha} \text{ } \Big ) N_{\ell} \geq \psi \Big( \text{ }\underbrace{\frac{N^+_{\ell'}}{N_{\ell'}}}_{a} \text { }\Big) \underbrace{\frac{N_{\ell'}}{N_{\ell}}}_{\alpha}N_{\ell} + \psi \Big( \text{ } \underbrace{\frac{N^+_{\ell''}}{N_{\ell''}}}_{b} \text{ } \Big ) \underbrace{\frac{N_{\ell''}}{N_\ell}}_{1-\alpha} N_{\ell} =$
	$= \underbrace{\psi \Big ( \frac{N^+_\ell}{N_\ell} \Big)N_\ell}_{\text{contribution of }\ell} \geq \underbrace{\psi \Big(\frac{N^+_{\ell'}}{N_{\ell'}} \Big )N_{\ell'}}_{\text{contribution of }\ell'} + \underbrace{\psi \Big ( \frac{N^+_{\ell''}}{N_{\ell''}}\Big )N_{\ell''}}_{\text{contribution of }\ell''}$
	meaning that a split never increases the training error (recall that $N_{\ell'}^+ + N_{\ell''}^+ = N_{\ell}^+$).
11) **Write the formulas for at least two splitting criteria $\psi$ used in practice to build tree classifiers.**
	Some examples of functions $\psi$ used in practice are:
	1) Gini function: $\psi_2(p) = 2p(1-p)$;
	2) scaled entropy: $\psi_3(p) = -\frac{p}{2}\log_2(p) - \frac{1-p}{2}\log_2(1-p)$;
	3) $\psi_4(p) = \sqrt{p(1-p)}$.
12) **Write the formula for the statistical risk of a predictor $h$ with respect to a generic loss function and data distribution.**
	The performance of a predictor $h : \mathcal{X} \to \mathcal{Y}$ with respect to the pair $(\mathcal{D}, \ell)$, where $\mathcal{D}$ is an uknown generic data distribution and $\ell$ is a known generic loss function, is evaluated via the statistical risk, deﬁned by $\ell_{\mathcal{D}}(h) = \mathbb{E}[\ell(Y, h(X))]$. This is the expected value of the loss function on a random example $(X, Y)$ drawn from $\mathcal{D}$.
13) **Write the formula for the Bayes optimal predictor for a generic loss function and data distribution.**
	The best possible predictor $f^*: \mathcal{X} \to \mathcal{Y}$ with respect to the pair $(\mathcal{D}, \ell)$ is known as Bayes optimal predictor, and is deﬁned by $f^*(x) = \underset{\widehat{y} \in \mathcal{Y}}{\operatorname{argmin}} \text{ } \mathbb{E}[\ell(Y, \widehat{y}) \vert X = x]$ where $\widehat{y} \in \mathcal{Y}$ is the value of the prediction for which the conditional risk $\mathbb{E}[\ell(Y, \widehat{y}) \vert X = x]$ attains its minimum.
14) **Write the formula for Bayes optimal predictor and Bayes risk for the zero-one loss.**
	Let $\eta(x) = \mathbb{P}( Y = +1 \text{ } \vert \text{ } X = x)$ be the value on $x$ of a function $\eta: \mathcal{X} \to [0, 1]$. Let $\mathbb{I}\{A\} \in \{0, 1\}$ be the **indicator function** of an event $A$, that is, $\mathbb{I}\{A\} = 1$ if and only if $A$ occurs.<br /> The statistical risk with respect to the zero-one loss $\ell(y, \widehat{y}) = \mathbb{I}\{\widehat{y} \neq y\}$ is therefore defined by $\ell_{\mathcal{D}}(h) = \mathbb{E}\big[\ell(Y, h(X))\big] = \mathbb{E}\big[\mathbb{I}\{h(X) \neq Y \}\big] =$ 
	$= 1 \cdot \mathbb{P}(h(X) \neq Y) + 0 \cdot \mathbb{P}(h(X) = Y) =  \mathbb{P}(h(X) \neq Y)$<br />
	The Bayes optimal predictor $f^*: \mathcal{X} \to \{−1, 1\}$ for binary classiﬁcation is derived as follows $\quad f^*(x) = \underset{\widehat{y} \in \{-1, 1\}}{\operatorname{argmin}}\mathbb{E}\big[\ell(Y, \widehat{y}) \text{ } \vert \text{ } X = x\big]$
	$= \underset{\widehat{y} \in \{-1, 1\}}{\operatorname{argmin}}\mathbb{E}\Big[\mathbb{I}\big\{Y = +1\big\}\mathbb{I}\big\{\widehat{y} = -1\big\} + \mathbb{I}\big\{Y = -1\big\}\mathbb{I}\big\{\widehat{y} = +1\big\} \text{ } \vert \text{ } X = x\Big]$
	$= \underset{\widehat{y} \in \{-1, 1\}}{\operatorname{argmin}}\Big (  \mathbb{E}\big[\mathbb{I}\{Y = +1\} \vert X = x\big]\mathbb{I}\{\widehat{y} = -1\} +  \mathbb{E}\big[\mathbb{I}\{Y = -1\} \vert X = x\big]\mathbb{I}\{\widehat{y} = +1\}\Big )$
	$= \underset{\widehat{y} \in \{-1, 1\}}{\operatorname{argmin}}\Big(\mathbb{P}\big(Y =  + 1 \text{ } \vert \text{ } X = x\big)\mathbb{I}\{\widehat{y} = -1\} + \mathbb{P}\big(Y = -1 \text{ } \vert \text{ } X = x\big)\mathbb{I}\{\widehat{y} = +1\}\Big)$
	$=\underset{\widehat{y} \in \{-1, 1\}}{\operatorname{argmin}}\Big(\eta(x) \mathbb{I}\{\widehat{y}= -1\} + (1 - \eta(x))\mathbb{I}\{\widehat{y} = +1\}\Big)$<br />
	If $\widehat{y} = 1$, the first term goes to $0$ and we predict the second term. Viceversa, we predict the first term. The algorithm should, then, predict $-1$ when $\eta(x) < (1 - \eta(x))$. So $f^*(x)= \cases{-1 \quad \text{if } \eta(x) < \frac{1}{2} \cr \cr +1 \quad \text{if } \eta(x) \geq \frac{1}{2}}$<br />
	Hence, the Bayes optimal classifier predicts the label whose probability is the highest when conditioned on the instance. Finally, it is easy to verify that the Bayes risk in this case is $\ell_{\mathcal{D}}(f^*) = \mathbb{E}[\ell(f^*(X), Y)] = \mathbb{E}\big[\mathbb{I}\{f^*(X) \neq Y\}\big] = \mathbb{P}(f^*(X) \neq Y)$<br />
		Knowing that $\mathbb{P}(f^*(X) \neq Y \vert X = x) = \operatorname{min} \{\eta(x), (1 - \eta(x))\}$ and knowing that $\mathbb{E}\Big[\mathbb{E}\big[\mathbb{I}\{f^*(x) \neq Y\}\big \vert X = x]\Big] = \mathbb{E}[\mathbb{I}\{f^*(x) \neq Y\}\big]$, it is possible to define $\ell_{\mathcal{D}}(f^*) = \mathbb{E}\big[\operatorname{min}\{\eta(X), 1 - \eta(X)\}\big]$.<br /><br />
	That is, the Bayes risk is the expectation of the probability of the event that is less likely to happen conditioned on the instance.
15) **Can the Bayes risk for the zero-one loss be zero? If yes, then explain how.**
	
16) **Write the formula for Bayes optimal predictor and Bayes risk for the square loss.**
	The Bayes optimal predictor for the square loss is $f^*(x) = \mathbb{E}\big[Y \text{ } \vert \text{ } X = x\big]$ and the Bayes risk for the square loss is $\ell_{\mathcal{D}}(f^*) = \mathbb{E}\Big[ Var\big[Y \vert X \big]\Big]$.<br />
	We now compute the Bayes optimal predictor $f^*$ for the quadratic loss function $\ell(y, \widehat{y}) = (y − \widehat{y})^2$ when $\mathcal{Y} \equiv \mathbb{R}$, $f^*(x) = \underset{\widehat{y} \in \mathbb{R}}{\operatorname{argmin}} \mathbb{E}\Big[(Y - \widehat{y})^2 \text{ } \vert \text{ } X = x\Big]$<br />
	$= \underset{\widehat{y} \in \mathbb{R}}{\operatorname{argmin}} \mathbb{E}\Big[Y^2 + \widehat{y}^2 -2\widehat{y}Y \text{ } \vert \text{ } X = x\Big]$<br />
	Because we are interested in the minimizing of $\widehat{y}$, it is possible to notice that the term $Y^2$ is a costant and, in fact, it does not depend on $\widehat{y}$. Using the linear property of expectation $\mathbb{E}[aX + bY] = a\mathbb{E}[X] + b\mathbb{E}[Y], \quad a, b \in \mathbb{R}$, we are able to write $= \underset{\widehat{y} \in \mathcal{Y}}{\operatorname {argmin}}\Big(\underbrace{\mathbb{E}\big[Y^2 \text{ }\vert \text{ }X = x \big]}_{\text{does not depend on }\widehat{y}} + \widehat{y}^2 - 2 \widehat{y} \mathbb{E} \big[Y \text{ }\vert \text{ }X = x\big] \Big)$<br />
	$= \underset{\widehat{y} \in \mathbb{R}}{\operatorname {argmin}}\Big( \widehat{y}^2 - 2\widehat{y} \mathbb{E}\big[Y \vert X = x\big]\Big)\quad \text{ } \quad (\text{ignoring the term that does not depend on } \widehat{y})$<br />
	and the reason why this is possible is because the term ignored is just a costant and so it is possible to extract it from the $\operatorname{argmin}$ operator.<br />
	Now, we have to minimize the function $F(\widehat{y}) = \widehat{y}^2 - 2\widehat{y}\mathbb{E}\big[Y \text{ } \vert \text{ } X = x\big]$, which can be seen as $F(z) = z^2 - 2zc$. Its derivative is $F'(z) = 2z -2c$ and it is minimized for $F'(z) = 0$. After the substitution in the derivative, we obtain $2 \widehat{y} - 2c = 2 \widehat{y} - 2 \mathbb{E}\big[Y \vert X = x\big] = 0$ and, therefore 
	$f^*(x) = \mathbb{E}\big[Y \text{ } \vert \text{ } X = x\big]$<br />
	Thus, the Bayes optimal prediction for the quadratic loss function is the expected value of the label conditioned on the instance. So, if it is desired to minimize the loss, the expectation must be predicted.
	Substituting in the conditional risk formula $\mathbb{E}[(Y - f^*(X))^2 \vert X = x]$ the Bayes optimal predictor $f^*(X) = \mathbb{E}[Y \text{ }\vert \text{ } X = x]$ we obtain
	$\mathbb{E}\Big[\big(Y - f^*(X)\big)^2 \text{ }\Big \vert \text{ } X = x \Big] = \mathbb{E}\Big[\big(Y - \mathbb{E}[Y \text{ } \vert \text{ } X = x ]\big)^2 \text{ }\Big \vert \text{ } X = x \Big] = Var\big[Y \vert X = x\big]$<br />
	In words, the conditional risk of the Bayes optimal predictor for the quadratic loss is the **conditional variance**, the variance of the label conditioned on the instance. By averaging over $X$ we obtain $\ell_{\mathcal{D}}(f^*) = \mathbb{E}\big[Var[Y \text{ } \vert \text{ } X]\big]$
17) **Explain in mathematical terms the relationship between test error and statistical risk.**
	We can estimate $\ell_{\mathcal{D}}(h)$ with the **test error**, which is the average loss of the predictor $h: \mathcal{X} \to \mathcal{Y}$ on the test set $S'$, that is, $\ell_{s'}(h) = \frac{1}{n}\sum_{t = 1}^{n}\ell\big(y_{t}', h(x_t')\big)$.<br />
	Under the assumption that the test set is generated through independent draws from $\mathcal{D}$, the test error corresponds to the **sample mean** of the statistical risk. Indeed, for each $t = 1, ... , n$ the example $(X_t', Y_t')$ is an independent draw from $\mathcal{D}$. Therefore, $\mathbb{E}\Big[\ell\big(Y_t', h(X_t')\big)\Big] = \ell_{\mathcal{D}}(h) \quad \quad t= 1, ..., n$<br />
	Note that the above equalities rely on the assumption that $h$ does not depend on the test set $S'$ (but it depends on the training set). If it did, then the above equalities would not be necessarily true.
18) **State the Chernoff-Hoeffding bounds.**
	Let $Z_1 , ... , Z_n$ be **independent and identically distributed random variables** with expectation $\mu$ and such that $0 \leq Z_t \leq 1$ for each $t = 1, ... , n$. Then, for any given $\varepsilon > 0$ <br />
	$\mathbb{P}\Bigg(\frac{1}{n}\sum_{t = 1}^{n}Z_t > \mu + \varepsilon \Bigg) \leq e^{-2\varepsilon^2n}\quad \text{and}\quad \mathbb{P}\Bigg(\frac{1}{n}\sum_{t = 1}^{n}Z_t < \mu - \varepsilon\Bigg) \leq e^{-2\varepsilon^2n}$<br />
	This means that if we compute the sample mean of the variables, the probability that this will be much larger (or much smaller) than the expectation, with respect to the draw of the sample, decreases exponentially with the sample size.
19) **Write the bias-variance decomposition for a generic learning algorithm $A$ and associate the resulting components to overfitting and underfitting.**
	Fix a finite training set $S$ and let $h_S = A(S)$. The following is called the **bias-variance** decomposition:<br />
	$\ell_{\mathcal{D}}(h_S) = \ell_{\mathcal{D}}(h_S) - \ell_{\mathcal{D}}(h^*)\quad\text{estimation (or variance) error (large when overfitting)}$<br />
	$\quad + \ell_{\mathcal{D}}(h^*) - \ell_{\mathcal{D}}(f^*)\quad \text{approximation (or bias) error (large when underfitting)}$<br />
	$\quad + \ell_{\mathcal{D}}(f^*) \quad \text{Bayes error (unavoidable)}$<br />
	where $f^*$ is the Bayes optimal predictor for $(\mathcal{D}, \ell)$. 
20) **Write the upper bound on the estimation error of ERM run on a finite class $\mathcal{H}$ of predictors. Bonus points if you justify your answer with a proof.**
	We study the case $\vert \mathcal{H} \vert < \infty$, that is when the model space contains a finite number of predictors. Note that the event $\exists h \in \mathcal{H} : \vert \ell_{\mathcal{D}}(h) - \ell_S(h) \vert > \varepsilon/2$ is the union over $h \in \mathcal{H}$ of the events $\vert \ell_{\mathcal{D}}(h) - \ell_S(h) \vert > \varepsilon/2$. Therefore, by the union bound, we have that its probability is bounded by $\vert \mathcal{H} \vert$ times the probability of the event $\vert \ell_{\mathcal{D}}(h) - \ell_S(h) \vert > \varepsilon/2$ for the predictor $h \in \mathcal{H}$ for which this probability is maximized. Therefore, its probabilty is bounded by $\vert \mathcal{H} \vert 2 e^{-m\varepsilon^2/2}$. In conclusion, knowing that $\mathbb{P}\big(\ell_{\mathcal{D}}(h_S) - \ell_{\mathcal{D}}(h^*) > \varepsilon\big) \leq \mathbb{P} \Big(\exists h \in \mathcal{H}: \vert \ell_S(h) - \ell_{\mathcal{D}}(h) \vert > \frac{\varepsilon}{2}\Big)$, we have that $\mathbb{P}\big(\ell_{\mathcal{D}}(h_S) - \ell_{\mathcal{D}}(h^*) > \varepsilon \big) \leq 2 \vert \mathcal{H} \vert e^{-m \varepsilon^2/2}$. Setting the right-hand side of the equation to $\delta$ and solving for $\varepsilon$, we obtain that $\ell_{\mathcal{D}}(h_S) \leq \ell_{\mathcal{D}}(h^*) + \sqrt{\frac{2}{m}\ln{\frac{2 \vert \mathcal{H} \vert}{\delta}}}$ with probability at least $1 − \delta$ with respect to the random draw of the training set. This implies that when the cardinality of the training set $m$ is suﬃciently large with respect to $\ln{\vert \mathcal{H} \vert}$, then the training error $\ell_S(h)$ becomes a good estimate for the statistical risk $\ell_{\mathcal{D}}(h)$ simultaneously for all predictors $h \in \mathcal{H}$.
21) **Write the upper bound on the estimation error of ERM run on the class of complete binary tree predictors with at most $N$ nodes on $d$ binary features.**
	Knowing that $\vert \mathcal{H}_N \vert \leq (2de)^N$ obtained via $\frac{N-1}{2}$-th Catalan number and knowing that the upper bound on the estimation error of ERM run on a finite class of predictors is $\ell_{\mathcal{D}}(h_S) \leq \ell_{\mathcal{D}}(h^*) + \sqrt{\frac{2}{m}\ln{\frac{2 \vert \mathcal{H} \vert}{\delta}}}$, we can write the upper bound on the estimation error of ERM run on a class of complete binary tree predictors with at most $N$ nodes on $d$ binary features substituting the bound on the cardinality of $\mathcal{H}$ in the inequality: if $h_s = \underset{\mathcal{H}_N}{\operatorname{argmin}}\ell_S(h)$ for a given $N$ and training set $S$, $\ell_{\mathcal{D}}(h_S) \leq \ell_{\mathcal{D}}(h^*) + \sqrt{\frac{2}{m}\Big(N(1 + \ln{(2d)}) + \ln{\frac{2}{\delta}}\Big)}$.<br />
	From that. we deduce that on this case a training set of size of order $N \ln{d}$ is enough to control the risk of $h_s \in \mathcal{H}_N$.
22) **Write the bound on the difference between risk and training error for an arbitrary complete binary tree classifier $h$ on $d$ binary features in terms of its number $N_h$ of nodes. Bonus points if you provide a short explanation on how this bound is obtained.**
	With probability at least $1 - \delta$ with respect to the training set random draw, $\ell_{\mathcal{D}}(h) \leq \ell_S(h) + \sqrt{\frac{1}{2m}\Big(\vert \sigma(h) \vert + \ln{\frac{2}{\delta}}\Big)}$<br />
	Hence, with Occam Razor given two predictor with the same training error the simpler (the shortest $\vert \sigma \vert$) is the best.<br />
	We upper bound the risk of a tree predictor $h$ by its training error plus a quantity $\varepsilon_h$ that now depends on the size of the tree.<br />
	We introduce a function $w: \mathcal{H} \to [0, 1]$ and call $w(h)$ the weight of tree predictor $h$. We assume $\sum_{h \in \mathcal{H}} w(h) \leq 1$.<br />
	And now choosing $\varepsilon_h = \sqrt{\frac{1}{2m}\Big(\ln{\frac{1}{w(h)}} + \ln{\frac{2}{\delta}}\Big)}$, we get that $\mathbb{P}(\exists h \in \mathcal{H}) : \vert \ell_{\mathcal{D}}(h) - \ell_S(h) \vert > \varepsilon_h \geq \sum_{h \in \mathcal{H}} \delta w(h) \leq \delta$.<br />
	A consequence of this analysis is that, with probability at least $1 - \delta$ with respect to the training set random draw, we have that $\ell_{\mathcal{D}}(h) \leq \ell_S(h) + \sqrt{\frac{1}{2m}\Big(\ln{\frac{1}{w(h)}} + \ln{\frac{2}{\delta}}\Big)}$.<br />
	Now, using a theoretic technique we can encode each tree predictor $h$ as a binary string $\sigma(h)$ of length $\vert \sigma(h) \vert = \mathcal{O}(N_h \log{d})$. Thanks to Kraft inequality, we can associate a weight $w(h) = 2^{-\vert \sigma(h) \vert}$ to each tree predictor $h$ in order to get the bound shown at the beginning.
23) **Write the formula for the $K$-fold cross validation estimate. Explain the main quantities occurring in the formula.**
	The **$K$-fold CV estimate** of $\mathbb{E}\big[\ell_{\mathcal{D}}(A)\big]$ on $S$, denoted by $\ell_S^{CV}(A)$, is computed as follows: we run $A$ on each training part $S_{-i}$ of the folds $i = 1, ..., K$ and obtain the predictors $h_1 =A(S_{-1}), ..., h_K =  A(S_{-K})$. We then compute the (rescaled) errors on the testing part of each fold, $\ell_{S_i}(h_i) = \frac{K}{m} \sum_{(x, y) \in  S_i} \ell(y, h_i(x))$.<br />
	Finally, we compute the CV estimate by averaging these errors $\ell_{S}^{CV}(A) = \frac{1}{K} \sum_{i = 1}^{K} \ell_{S_i}(h_i)$.
24) **Write the pseudo-code for computing the nested cross validation estimate.**
	![[K-fold Nested Cross Validation.png]]
25) **Write the mathematical definition of consistency for an algorithm $A$.**
	A learning algorithm $A$ is **consistent** with respect to a loss function $\ell$ if for any data distribution $\mathcal{D}$ it holds that $\underset{m \to \infty}{\operatorname{lim}}\mathbb{E}\Big[\ell_{\mathcal{D}}(A(S_m))\Big] = \ell_{\mathcal{D}}(f^*)$ where the expectation is with respect to the random draw of the training set $S_m$ of size $m$ from the distribution $\mathcal{D}$, and $\ell_{\mathcal{D}}(f^*)$ is the Bayes risk for $(\mathcal{D}, \ell)$.
26) **Write the statement of the no-free-lunch theorem.**
	For any sequence $a_1 , a_2 , . . .$ of positive numbers converging to zero and such that $\frac{1}{16}≥ a_1 ≥ a_2 ≥ · · ·$ and for any consistent learning algorithm $A$ for binary classification with zero-one loss, there exists a data distribution $\mathcal{D}$ such that $\ell_{\mathcal{D}}(f^*) = 0$ and $\mathbb{E}\Big[\ell_{\mathcal{D}}(A(S_m))\Big] \geq a_m \quad \forall m \geq 1$.<br />
	Theorem of No Free Lunch does not prevent a consistent algorithm from converging fast to the Bayes risk for specific distributions $\mathcal{D}$. What the theorem shows is that if $A$ converges to the Bayes risk for any data distribution, then it will converge arbitrarily slow for some of these distributions.
27) **Write the mathematical definition of nonparametric learning algorithm. Define the main quantities occurring in the formula.**
	We say that $A$ is a **nonparametric learning algorithm** if $A$'s approssimation error vanishes as $m$ grows to infinity. Formally, $\underset{m \to \infty}{\operatorname{lim}}\underset{h \in \mathcal{H}_m}{\operatorname{min}}\ell_{\mathcal{D}}(h^*) = \ell_{\mathcal{D}}(f^*)$.
28) **Name one nonparametric learning algorithm and one parametric learning algorithm.**
	Some examples of nonparametric learning algorithms are $k$-nearest neighboors and greedy decision tree classifiers. Some examples of parametric learning algorithms are linear regression and logistic regression.
29) **Write the mathematical conditions on $k$ ensuring consistency for the $k-NN$ algorithm.**
	The standard $k-NN$ algorithm is nonparametric but not known to be consistent for any fixed value of $k$. Indeed, one can only show that $\underset{m \to \infty}{\operatorname{lim}}\mathbb{E}\Big[\ell_{\mathcal{D}}(k-NN(S_m))\Big] \leq \ell_{\mathcal{D}}(f^*) + 2 \sqrt{\frac{\ell_{\mathcal{D}}(f^*)}{k}}$ for any data distribution $\mathcal{D}$. However, if we let $k$ be chosen as a function $k_m$ of the training set size, then the algorithm becomes consistent provided $k_m \to \infty$ and $k_m = o(m)$.
30) **Write the formula for the Lipschitz condition in a binary classification problem. Define the main quantities occurring in the formula.**
	We may define consistency with respect to a restricted class of distributions $\mathcal{D}$. For example, in binary classification we may restrict to all distributions $\mathcal{D}$ such that $\eta(x) = \mathbb{P}(Y = 1 \vert X = x)$ is a Lipschitz function on $\mathcal{X}$. Formally, there exists $0 < c < \infty$ such that $\vert \eta(x) - \eta(x')\vert \leq c \Vert x - x' \Vert \quad \text{ for all } x, x' \in \mathcal{X}$. Technically, this conditions implies that $\eta$ is Lipschitz in $\mathcal{X}$. This is a restriction on the set of all allowed $\eta$ as $c < \infty$ implies continuity (but the opposite is not true).
31) **Write the rate at which the risk of a consistent learning algorithm for binary classification vanish as a function of the training set size $m$ and the dimension $d$ under Lipschitz assumptions.**
	Under Lipschitz assumptions on $\eta$, the typical convergence rate to Bayes risk is $m^{−1/(d+1)}$ (exponentially slow in $d$).
32) **Explain the curse of dimensionality.**
	The convergence rate $m^{\frac{-1}{d+1}}$ implies that to get $\varepsilon$-close to the Bayes Risk, we need a training set size $m$ of order $\varepsilon^{-(d+1)}$. This exponential dependence on the number of features of the training set size is known a curse of dimensionality and refers to the difficulty of learning in a nonparametric setting when datapoints live in a high-dimensional space.
33) **Why non parametric algorithm cannot be consistent? (BONUS)**
34) **Write the bound on the risk of $1-NN$ binary classifier under Lipschitz assumptions.**
	$\mathbb{E}\Big[\ell_{\mathcal{D}}(A(S_m))\Big] \leq 2 \ell_{\mathcal{D}}(f^*) + \varepsilon_m$, where $A$ denotes the $1-NN$ algorithm, $S_m$ the training set of size $m$, $\ell_{\mathcal{D}}(f^*)$ is the Bayes risk, and $\varepsilon_m$ is a quantity that vanishes for $m \to \infty$.
35) **Can the ERM over linear classifiers be computed efficiently? Can it be approximated efficiently? Motivate your answers.**
	
36) **Write the system of linear inequalities stating the condition of linear separability for a training set for binary classfication.**
	A training set $(x_1, y_1), ..., (x_m, y_m)$ is linearly separable where here exists a linear classifier with zero training error. In other words, there exists a separating hyperplane $u \in \mathbb{R}^d$ such that $\gamma(u) \overset{\text{def}}{=}\underset{t = 1, ..., m}{\operatorname{min}} y_t u^{\top}x_t > 0$.<br />
	We can write the condition of linear separability as a system of linear inequalities, and this solution can be found in polynomial time using an algorithm for linear programming $\cases{ u^\top x_t > 0 \quad \text{ if }t = 1 \cr \cr ... \quad \text{ } \quad \text{ } \quad \text{ }... \cr \cr u^\top x_t > 0 \quad \text{ if } t = m}$
37) **Write the pseudo-code for the Perceptron algorithm.**
	![[Perceptron Algorithm.png]]
38) **Write the statement of the Perceptron convergence theorem.
	(**Convergence of Perceptron**): let $(x_1, y_1), ...,  (x_m, y_m)$ be a linearly separable training set. Then the Perceptron algorithm terminates after a number of updates not bigger than $\Bigg(\underset{u: \gamma(u) \geq 1}{\operatorname{min}} \Vert u \Vert^2 \Bigg)\Bigg( \underset{t = 1, ..., m}{\operatorname{max}} \Vert x_t \Vert^2 \Bigg)$<br />
	The apparently stonger margin constraint $\gamma(u) \geq 1$ is actually achievable by any separating hyperplane $u$. Indeed, if $\gamma(u) > 0$, then $y_tu^{\top}x_t \geq \gamma(u)$ is equivalent to $y_tv^{\top}x_t \geq 1$ for $v = u/\gamma(u)$. Hence, $\gamma(u) \geq 1$ can be achieved simply by rescaling $u$.<br />
	**Proof**: let $w_0 = (0, ..., 0)$ be the initial hyperplane. Let $w_M$ be the hyperplane after $M$ updates and let $t_M \in \{1, ..., m\}$ be the index of the training example $(x_{t_M}, y_{t_M})$ that caused the $M$-th update $w_M = w_{M −1} + y_{t_M} x_{t_M}$. We prove an upper bound on $M$ by deriving upper and lower bounds on $\Vert w_M \Vert \Vert u \Vert$. We start by observing that<br />
	$\Vert w_M \Vert^2 = \Vert w_{M−1} + y_{t_M} x_{t_M} \Vert^2 = \Vert w_{M −1} \Vert^2 + \Vert x_{t_M} \Vert^2 + 2y_{t_M}w^{\top}_{M −1}x_{t_M} \leq \Vert w_{M-1} \Vert^2 + \Vert x_{t_M} \Vert^2$<br />
	because $y_{t_M} w^{\top}_{M −1}x_{t_M} \leq 0$ due to the update $w_M = w_{M −1} + y_{t_M} x_{t_M}$. Iterating this argument $M$ times, and recalling that $w_0 = (0, ...,  0)$, we obtain<br />
	$\Vert w_M \Vert^2 \leq \Vert w_0\Vert^2 + \sum_{i = 1}^{M} \Vert x_{t_i} \Vert^2 \leq M \Big( \underset{t = 1, ...., m}{\operatorname{max}} \Vert x_t \Vert^2 \Big )$.<br />
	Hence
	$\Vert w_M \Vert \Vert u \Vert \leq \Vert u \Vert \Big (\underset{t = 1, ..., m}{\operatorname{max}} \Vert x_t \Vert \Big)\sqrt{M}$.<br />
	To prove the lower bound, fix any separating hyperplane $u$ with $\gamma(u) \geq 1$ and let $\theta$ be the angle between $u$ and $w_M$. We have
	$\Vert w_M \Vert \Vert u \Vert \geq \Vert w_M \Vert \Vert u \Vert \cos(\theta)\quad \text{ } \quad (\text{since } − 1 \leq \cos(\theta) \leq 1)$<br />
	$= w^{\top}_M u \quad \text{ } \quad \text{(by definition of inner product)}$<br />
	$= (w_{M−1} + y_{t_M} x_{t_M} )^{\top} u$<br />
	$= w^{\top}_{M −1}u + y_{t_M} u^{\top}x_{t_M}$<br />
	$\geq w^{\top}_{M−1}u + 1$<br />
	where the last inequality holds because $1 \leq \gamma(u) \leq y_tu^{\top}x_t$ for all $t = 1, ..., m$. Iterating $M$ times we get<br />
	$\Vert w_M \Vert \Vert u \Vert \geq w^{\top}_0 u + M = M$<br />
	where we used $w^{\top}_0u = 0$ since $w_0 = (0, ..., 0)$. Combining upper and lower bound we obtain<br />
	$M \leq \Vert u \Vert \Big ( \underset{t = 1, ..., m}{max} \Vert x_t \Vert \Big )\sqrt{M}$<br />
	Solving for $M$, and recalling the choice of $u$, we obtain $(2)$. Hence, the update count $M$ cannot grow larger than $(2)$. Since the algorithm stops when no more updates are possible, we conclude that the Perceptron terminates after a bounded number of updates.<br />
	Note that the Perceptron convergence theorem does not imply that the Perceptron algorithm terminates in polynomial time on any linearly separable training set. Indeed, it can be shown that the bound $(2)$ is tight in any fixed dimension $d \geq 2$. Hence, although each update takes constant time $\Theta(d)$, the number of updates can still be exponential in $md$ whenever $\gamma(u) \geq 1$ only for those $u$ whose length $\Vert u \Vert$ is very big. Or, equivalently, when the margin $\gamma(u)$ is very small for any linear separator $u$ such that $\Vert u \Vert = 1$.
39) **Write the closed-form formula (i.e., not the argmin definition) for the Ridge Regression predictor. Define the main quantities occurring in the formula.**
	The closed-form formula for the Ridge Regression coefficients (predictor) is $w = w_{S, \alpha} = (S^\top S + \alpha I)^{-1} S^\top y$, where $S$ is the matrix of training examples of size $m \times d$ called design matrix, $y$ is the vector of labels of size $m \times 1$, $\alpha$ is the regularization parameter or penalty factor that shrinks the regression coefficients towards zero and $w$ is the vector of Ridge Regression coefficients of length $d$.
40) **Write the pseudo-code for the projected online gradient descent algorithm.**
	![[Projected OGD.png]]
41) **Write the upper bound on the regret of projected online gradient descent on convex functions. Define the main quantities occurring in the bound.**
	$\underbrace{\frac{1}{T}\sum_{t = 1}^{T}\ell_t(w_t)}_{A} \leq \underbrace{\underset{u : \Vert u \Vert \leq U}{\operatorname{min}} \frac{1}{T} \sum_{t = 1}^{T} \ell_t(u)}_{B} + UG\sqrt{\frac{8}{T}}$<br />
	- $A$ is the sequential risk of a sequence of predictors generated by an online algorithm;
	- $B$ is the sequential risk of the predictor $u$ in the ball of radius $U$ with smallest average loss over the first $T$ steps;
	- $UG\sqrt{\frac{8}{T}}$ is an upper bound of the regret of the online alogrithm;
	- $U$ is the radius of the Euclidean sphere;
	- $G$ is the bound on the norm of the gradient of the loss function;
	- $T$ is the number of training points.
42) **Write the upper bound on the regret of online gradient descent on $\sigma$-strongly convex functions. Define the main quantities occurring in the bound.**
	$\underbrace{\frac{1}{T}\sum_{t = 1}^{T}\ell_t(w_t)}_{\ell_T(w)} \leq \underset{u \in \mathbb{R}^d}{\operatorname{min}} \frac{1}{T} \sum_{t = 1}^{T} \ell_t(u) + \underbrace{\frac{G^2}{2\sigma}\frac{\ln{(T + 1)}}{T}}_{\frac{R_T(u)}{T} = \text{Regret}}$<br />
	where
	- $\ell_T(w)$ is the average loss of the algorithm, 
	- $\sigma$ is the strong convexity parameter and it must be $> 0$;
	- $G$ is the bound on the norm of the gradient of the loss function $\underset{t}{\operatorname{max}} \Vert \nabla \ell_t(w_t) \Vert \leq G$;
	- $\ln{(T + 1)}$ is the upper bound of harmonic series over $T$;
	- $T$ is the number of epochs.
43) **Write the formula for the hinge loss.**
	The function $h_t(u) = [1 - y_t u^{\top}x_t]_+$ is a loss function called **hinge loss**, which is a convex function and, since $\mathbb{I}\{\operatorname{sgn}(z) \neq y\} \leq [1− zy]_+$ for all $z \in \mathbb{R}$ and $y \in \{−1,1\}$, it is also a convex upper bound on the zero-one loss.
44) **Write the mistake bound for the Perceptron run on an arbitrary data stream for binary classification. Define the main quantities occurring in the bound.**
	$M \leq \sum_{t = 1}^{T}h_t(u) + \big(\Vert u \Vert X\big)^2 + \Vert u \Vert X \sqrt{\sum_{t = 1}^{T}h_t(u)} \quad \text{ for all } u \in \mathbb{R}^d$.<br />
	This shows a bound on the number of mistakes made by the Perceptron on any data sequence of arbitrary length $T$, including those sequences that are not linearly separable.
	- $u$ is a weight vector and not necessarily a separator, because we are not making any assumption on the stream;
	- $m$ is the number of mistakes made by the Perceptron algorithm;
	- $h_t(u)$ is the hinge loss at time $t$ for the weight vector $u$. When the sequence is linearly separable then there exists $u \in \mathbb{R}^d$ such that $y_tu^\top x_t \geq 1$ for all $t \leq T$, which in turn implies that $h_t(u) = 0$;
	- $X$ is the maximum norm of the data points, i.e. $\Vert x_t \Vert \leq X$ for all $t \leq T$.
45) **Write the formula for the polynomial kernel of degree $n$.**
	The polynomial kernel is $K_n(x,x') = (1 + x^{\top}x')^n$ for all $n \in \mathbb{N}$ We can also express it using Newton’s Binomial Theorem such that $K_n(x,x') = \phi_n(x)^{\top}\phi_n(x')$:<br />
	$K_n(x,x') = (1 + x^{\top}x')^n = \sum_{k = 0}^{n} \binom{n}{k}(x^{\top}x')^k$.
46) **Write the formula for the Gaussian kernel with parameter $\gamma$.**
	$K_{\gamma}(x, x') = \operatorname{exp}\Big(-\frac{1}{2\gamma} \Vert x - x'\Vert^2\Big) \quad \text{ } \quad \gamma > 0$, where $\gamma$ determines the width of the Gaussians centered in $x$. This equality can be manipulated into a linear combination of infinitely many polynomial kernels $(x^\top x')^n$ of increasing degree, each weighted by the reciprocal of the factorial of its degree and scaled by $\gamma$. Finally, the factors $e^{-\Vert x \Vert^2 / (2 \gamma)}e^{-\Vert x' \Vert^2/(2\gamma)}$ normalize with respect to $x$ and $x'$ giving $K_\gamma(x,x) = 1$ for each $x \in \mathbb{R}^d$.
47) **Write the pseudo-code for the kernel Perceptron algorithm.**
	![[Kernel Perceptron.png]]
48) **Write the mathematical definition of the linear space $\mathcal{H}_K$ of functions induced by a kernel $K$.**
	Since $\mathcal{H}_K$ must be a linear space, it is possible to define it as the set of all linear combinations of $K(x,\cdot)$ for arbitrary choices of the coefficients and of the points $x \in  \mathcal{X}$, $\mathcal{H}_K \equiv \Bigg \{\sum_{i = 1}^N \alpha_i K(x_i, \cdot) : x_1, ..., x_N \in \mathcal{X}, \alpha_i, ...., \alpha_N \in \mathbb{R}, N \in \mathbb{N} \Bigg\}$
49) **Let $f$ be an element of the linear space $\mathcal{H}_K$ induced by a kernel $K$. Write $f(x)$ in terms of $K$.**
	An element $f \in \mathcal{H}_K$ is a function $f : \mathcal{X} \to \mathbb{R}$ such that $f(x) = \sum_{i = 1}^{N}\alpha_iK(x_i, x)$ for some $x_1, ..., x_N \in \mathcal{X}, \alpha_1, ..., \alpha_N \in \mathbb{R}$, and $N \in \mathbb{N}$.
50) **Write the mistake bound of the Perceptron convergence theorem when the Perceptron is run with a kernel $K$. Define the main quantities occurring in the bound.**
	Recall the bound on the number of mistakes provided by the Perceptron convergence theorem, $\Vert u \Vert^2 \Big(\underset{t}{\operatorname{max}} \Vert x_t \Vert^2\Big)$ which holds for any $u \in \mathbb{R}^d$ such that $y_tu^{\top}x_t \geq 1$ for $t = 1, ..., m$, where $u$ is a weight vector and not necessarily a separator, because we are not making any assumption on the stream and $m$ is the number of mistakes made by the Perceptron algorithm.<br />
	In a generic reproducing kernel Hilbert space $\mathcal{H}_K$, the linear separator $u$ is some $g \in \mathcal{H}_K$ such that $y_tg(x_t) \geq 1$ for $t = 1, ..., m$. The squared norm $\Vert x_t \Vert^2 = x^{\top}_tx_t$ becomes $\Vert \phi_K(x)\Vert^2_K = \langle K(x,\cdot), K(x, \cdot)\rangle_K = K(x,x)$. Finally $\Vert u \Vert^2$ is replaced by $\Vert f \Vert^2_K = \Bigg \Vert \sum_{i = 1}^{N}\alpha_iK(x_i, \cdot)\Bigg \Vert^2_K = \Bigg \langle  \sum_{i = 1}^{N} \alpha_i K (x_i, \cdot), \sum_{j = 1}^{N}\alpha_jK(x_j, \cdot)\Bigg \rangle_K = \sum_{i, j = 1}^{N}\alpha_i \alpha_j K (x_i, x_j)$.
51) **Write the mistake bound for the kernel Perceptron run on an arbitrary data stream for binary classification. Define the main quantities occurring in the bound.**
	Recall the bound on the number of mistakes for the Perceptron run on an arbitrary data stream for binary classification:<br />$M \leq \sum_{t = 1}^{T}h_t(u) + \big(\Vert u \Vert X\big)^2 + \Vert u \Vert X \sqrt{\sum_{t = 1}^{T}h_t(u)} \quad \text{ for all } u \in \mathbb{R}^d$<br />
	In a generic reproducing kernel Hilbert space $\mathcal{H}_k$, the hyperplane $u$ is some $g \in \mathcal{H}_K$. $X$ was the maximum norm of the examples, which becomes $\underset{t}{\operatorname{max}} \Vert \phi_K(x_t) \Vert_K = \underset{t}{\operatorname{max}}K(x_t, \cdot)$.
52) **Write the closed-form formula (i.e., not the argmin definition) of the kernel version of the Ridge Regression predictor.**
	Recall the closed-form formula for the Ridge Regression predictor $w = \big( \alpha I + S^{\top}S\big)^{−1}S^\top y$ where $S$ is the $m \times d$ matrix of the training examples and $y$ is the vector of the labels.<br />
	We can represent the ridge regression predictor in a generic reproducing kernel Hilbert space $\mathcal{H}_K$ as by<br />
	$y^{\top}\big(\alpha I + K\big)^{-1}k(\cdot)$ where $K$ is the $m \times m$ matrix with entries $K_{i, j} = K(x_i, x_j)$ and $K(\cdot)$ is the vector $(K(x_1, \cdot), ..., K(x_m, \cdot))$ of functions $K(x_t, \cdot) = \langle \phi_K(x_t) \rangle$. Similarly to the non-kernel case, where the prediction on $x$ is $w^{\top}x$, the prediction on $\phi_K(x)$ is $\langle g, \phi_K(x_t),\cdot \rangle_K$ which evaluates to $g(x) = y^\top \Big( \alpha I + K\Big)^{−1}k(x)$ where $k(x) = (K(x_1, x), ..., K(x_m, x))$.
53) **Write the convex optimization problem with linear constraints that defines the SVM hyperplane in the linearly separable case.**
	Given a linearly separable training set $(x_1, y_1), ..., (x_m,y_) \in \mathbb{R}^d \times \{−1, 1\}$, SVM outputs the linear classifier corresponding to the unique solution $w^* \in \mathbb{R}^d$ of the following convex optimization problem with linear constraints $\underset{w \in \mathbb{R}^d}{\operatorname{min}} \frac{1}{2}\Vert w \Vert ^2$<br />
	$\text{s.t. }\quad y_tw^{\top}x_t \geq 1 \quad t = 1, ..., m$<br />
	Geometrically, $w^*$ corresponds to the maximum margin separating hyperplane. For every linearly separable set $(x_1, y_1), ..., (x_m, y_m) \in \mathbb{R}^d \times \{−1, 1\}$, the maximum margin is defined by <br />
	$\gamma^* = \underset{u: \Vert u \Vert = 1}{\operatorname{max}} \underset{t = 1, ..., m}{\operatorname{min}} y_t u^{\top}x_t$<br />
	and the vector $u^*$ achieving the maximum margin is the maximum margin separator.
54) **Write the unconstrained optimization problem whose solution defines the SVM hyperplane when the training set is not necessarily linearly separable.**
	
55) **Write the bound on the expected value of the SVM objective function achieved by Pegasos. Provide also a bound on the expected squared norm of the loss gradient.**
	
56) **Write the definition of $\varepsilon$-stability for a learning algorithm.**
	
57) **Write the value of $\varepsilon$ for which SVM is known to be stable. The value depends on the radius $X$ of the ball where the training datapoints live, the training set size $m$, and the regularization coefficient $\lambda$.**
	
58) **Write the mathematical conditions on the regularization coefficient $\lambda$ ensuring consistency for the SVM algorithm wih Gaussian kernel.**
	
59) **Consider the class $\mathcal{F}_d$ of all functions of the form $f : \{−1, 1\}^d \to \{−1, 1\}$. Let $\mathcal{F}_{G,\operatorname{sgn}}$ be the class of functions computed by a feedforward neural networks with the $\operatorname{sgn}$ activation function and graph $G = (V, E)$. Provide asymptotic upper and lower bounds on $\vert V \vert$ such that $\mathcal{F}_d \subseteq \mathcal{F}_{G,\operatorname {sgn}}$.**
	
60) **Define a class of neural networks for which the ERM problem with the square loss is probably NP-hard.**
	
61) **Write the update line of the stochastic gradient descent algorithm. Explain the main quantities.**
	
62) **Write the definition of logistic loss for logistic regression with linear models.**
	
63) **Write the definition of consistency for surrogate losses.**
	
64) **Write a sufficient condition for consistency of a surrogate loss.**
	
65) **Write the formula for Bayes optimal predictor and Bayes risk for the logistic loss.**
	