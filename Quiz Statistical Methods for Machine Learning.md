# Quiz list for the written test

- **Write the formula for the square loss, the zero-one loss, and the logarithmic loss.**
	The square loss formula is $\ell(y, \widehat{y}) = (y - \widehat{y})^2$;
	the zero-one loss is $\ell(y, \widehat{y}) = \cases{1 \quad \text{ if } y \neq \widehat{y} \cr \cr 0 \quad \text{ otherwise }}$
	the logarithmic loss is $\ell(y, \widehat{y}) = \cases{\ln{\frac{1}{\widehat{y}}} \quad \text{ if } y = 1 \cr \cr \ln{\frac{1}{1 - \widehat{y}}} \quad \text{ if } y = 0}$
- **What is the mathematical definition of predictor?(BONUS)**
	Given the set $\mathcal{X}$ of all possible data points for a given learning problem and given the set $\mathcal{Y}$ of all possible labels, a predictor is a function $f: \mathcal{X} \to \mathcal{Y}$ mapping data points to labels (or $f: \mathcal{X} \to \mathcal{Z}$ if the predictions belong to a set $\mathcal{Z}$ different from $\mathcal{Y}$).
- **Write the mathematical formula defining the training error of a predictor $h$.**
	The training error or empirical risk is $\ell_s(h) = \frac{1}{m}\sum_{t=1}^{m}\ell(y_t, h(\mathbf{x}_t))$.
- **What does a learning algorithm receive in input? And what does it produce in output?**
	A learning algorithm receives a training set in input and it produces a predictor in output. A training set $S$ is a set of examples $\{(x_1, y_1), ..., (x_m, y_m)\}$, where $x$ is a data point and $y$ is the corresponding label.
- **Write the mathematical formula defining the ERM algorithm over a class $\mathcal{H}$ of predictors. Define the main quantities occurring in the formula.**
	Given a class $\mathcal{H}$ of predictors and a loss function $\ell$, the Empirical Risk Minimizer is the learning algorithm that outputs some predictors minimizing the empirical risk on the training set $S$. The mathematical formula is $\widehat{f} \in \underset{f \in \mathcal{H}}{\operatorname{argmin}} \ell_S(f)$. The $\widehat{f} \in$ notation takes into account the fact that there could be multiple $f \in \mathcal{H}$ minimizing the training error.
- **How do you define overfitting and underfitting in terms of behavior of an algorithm on training and test set?**
	it is possible to give specific names to the two ways of failing for a generic learning algorithm $A$, that is, when it returns a predictor with high test error :
	- if $A$ fails by returning predictors with high training error, then we say that $A$ is **underfitting**;
	- if $A$ fails by returning predictors with low training error, then we say that $A$ is **overfitting**.
- **Name and describe three reasons why labels may be noisy.**
	Noise may occur for at least three (not mutually exclusive) reasons:
	1) **human in the loop**: the labels are assigned by a human annotator who decides the “true” label for each data point. In this case, different annotators may have different opinions;
	2) **epistemic uncertainty**: each data point is represented by a feature vector $x$ that does not contain enough information to uniquely determine the label;
	3) **aleatoric uncertainty**: the feature vector $x$ representing a data point is obtained through noisy measurements. The label associated with a given $x$ is then stochastic because the same $x$ could have been generated by different data points.
	Noisy labels cause overfitting because they may mislead the algorithm with regard to what is the “true” label for a given data point.
- **Write a short pseudo-code for the $k-NN$ algorithm for binary classification. (BONUS)**
	In order to compute $h_{k−NN}(x)$, the following operations are performed:
	1) find the $k$ training points $x_{t_1} ,..., x_{t_k}$ closest to $x$ (if there are more than $k$ training points closest to $x$, choose the $k'$ training points where $k'$ is the smallest integer bigger or equal to $k$ such that the $(k' +1)$-th data point has distance from $x$ strictly larger than the $k'$-th point). Let $y_{t_1} ,...,y_{t_k}$ be their labels;
	2) if the majority of the labels $y_{t_1} ,...,y_{t_k}$ is $+1$, then $h_{k−NN}(x) = +1$; if the majority is $−1$, then $h_{k−NN}(x) = −1$. If there is an equal number of closest points with positive and negative labels, then the algorithm predicts a default value in $\{−1,1\}$ (for instance, the most frequent label in the training set).
- **Is $k-NN$ more likely to overfit when $k$ is large or small?**
	The learning algorithm suffers from high test error for small values of $k$ (overfitting) and for large values of $k$ (underfitting).
- **Write a short pseudo-code for building a tree classifier based on a training set $S$.**
	1) **initialization**: create a tree $T$ with only the root $\ell$ and let $S_\ell = S$. Let the label associated with the root be the most frequent label in $S_\ell$;
	2) **main loop**: pick a leaf $\ell$ and replace it with an internal node $v$ creating two children $\ell'$ (first child) and $\ell''$ (second child). Pick an attribute $i$ and a test $f : \mathcal{X}_i \to \{1,2\}$. Associate the test $f$ with $v$ and partition $S_\ell$ in the two subsets $S_{\ell '} = \{(x_t, y_t) \in S_{\ell} : f(x_{t, i}) = 1\}$ and $S_{\ell''} = \{(x_t, y_t) \in S_{\ell} : f(x_{t, i}) = 2\}$. Let the labels associated with $\ell'$ and $\ell''$ be, respectively, the most frequent labels in $S_{\ell'}$ and $S_{\ell''}$;
- **What is the property of a splitting criterion $\psi$ ensuring that the training error of a tree classifier does not increase after a split? Bonus points if you justify your answer with a proof.**
	The property of a splitting criterion $\psi$ ensuring that the training error of a tree classifier does not increase after a split is the fact that $\psi$ is a concave function.
	To prove it, we can utilize Jensen's inequality, stating that $\psi(\alpha a + (1 -\alpha)b) \geq \alpha \psi (a) + (1 - \alpha) \psi (b) \quad \forall a, b \in \mathbb{R} \text{ and all } \alpha \in [0,1]$.<br />
	We can study how the training error changes when $\ell$ is replaced by two new leaves $\ell'$ and $\ell''$, $\underbrace{\psi \Big ( \frac{N^+_\ell}{N_\ell} \Big)N_\ell}_{\text{contribution of }\ell \text { to the training error}} = \psi \Big ( \frac{N^+_{\ell'}}{N_\ell} + \frac{N^+_{\ell''}}{N_\ell} \Big)N_\ell = \psi \Big ( \frac{N^+_{\ell'}}{N_\ell} \frac{N_{\ell'}}{N_{\ell'}} + \frac{N^+_{\ell''}}{N_\ell}\frac{N_{\ell''}}{N_{\ell''}} \Big)N_\ell =$
	At this point it is possible to apply Jensen's inequality: $= \psi \Big ( \text{ }\underbrace{\frac{N^+_{\ell'}}{N_{\ell'}}}_{a} \underbrace{\frac{N_{\ell'}}{N_\ell}}_{\alpha} + \underbrace{\frac{N^+_{\ell''}}{N_{\ell''}}}_{b} \underbrace{\frac{N_{\ell''}}{N_{\ell}}}_{1- \alpha} \text{ } \Big ) N_{\ell} \geq \psi \Big( \text{ }\underbrace{\frac{N^+_{\ell'}}{N_{\ell'}}}_{a} \text { }\Big) \underbrace{\frac{N_{\ell'}}{N_{\ell}}}_{\alpha}N_{\ell} + \psi \Big( \text{ } \underbrace{\frac{N^+_{\ell''}}{N_{\ell''}}}_{b} \text{ } \Big ) \underbrace{\frac{N_{\ell''}}{N_\ell}}_{1-\alpha} N_{\ell} =$
	$= \underbrace{\psi \Big ( \frac{N^+_\ell}{N_\ell} \Big)N_\ell}_{\text{contribution of }\ell} \geq \underbrace{\psi \Big(\frac{N^+_{\ell'}}{N_{\ell'}} \Big )N_{\ell'}}_{\text{contribution of }\ell'} + \underbrace{\psi \Big ( \frac{N^+_{\ell''}}{N_{\ell''}}\Big )N_{\ell''}}_{\text{contribution of }\ell''}$
	meaning that a split never increases the training error (recall that $N_{\ell'}^+ + N_{\ell''}^+ = N_{\ell}^+$).
- **Write the formulas for at least two splitting criteria $\psi$ used in practice to build tree classifiers.**
	Some examples of functions $\psi$ used in practice are:
	1) Gini function: $\psi_2(p) = 2p(1-p)$;
	2) scaled entropy: $\psi_3(p) = -\frac{p}{2}\log_2(p) - \frac{1-p}{2}\log_2(1-p)$;
	3) $\psi_4(p) = \sqrt{p(1-p)}$.
- **What are the elements of a learning problem in statistical learning? (BONUS)**
	In statistical learning, a **problem** is fully speciﬁed by a pair $(\mathcal{D}, \ell)$, where $\mathcal{D}$ is the data distribution and $\ell$ is a loss function.
- **Write the formula for the statistical risk of a predictor $h$ with respect to a generic loss function and data distribution.**
	The performance of a predictor $h : \mathcal{X} \to \mathcal{Y}$ with respect to $(\mathcal{D}, \ell)$, where $\mathcal{D}$ is a generic data distribution and $\ell$ is a generic loss function, is evaluated via the statistical risk, deﬁned by $\ell_{\mathcal{D}}(h) = \mathbb{E}[\ell(Y, h(X))]$. This is the expected value of the loss function on a random example $(X, Y)$ drawn from $\mathcal{D}$.
- **Write the formula for the Bayes optimal predictor for a generic loss function and data distribution.**
	The best possible predictor $f^*: \mathcal{X} \to \mathcal{Y}$ given $\mathcal{D}$ is known as Bayes optimal predictor, and is deﬁned by $f^*(x) = \underset{\widehat{y} \in \mathcal{Y}}{\operatorname{argmin}} \text{ } \mathbb{E}[\ell(Y, \widehat{y}) \vert X = x]$ where $\widehat{y} \in \mathcal{Y}$ is the value for which the conditional risk $\mathbb{E}[\ell(Y, \widehat{y}) \vert X = x]$ attains its minimum.
- **Write the formula for Bayes optimal predictor and Bayes risk for the zero-one loss.**
	Let $\eta(x)$ be the probability of $Y = 1$ conditioned on $X = x$. We view $\eta(x) = \mathbb{P}( Y = +1 \text{ } \vert \text{ } X = x)$ as the value on $x$ of a function $\eta: \mathcal{X} \to [0, 1]$. Let $\mathbb{I}\{A\} \in \{0, 1\}$ be the **indicator function** of an event $A$, that is, $\mathbb{I}\{A\} = 1$ if and only if $A$ occurs.<br /> The statistical risk with respect to the zero-one loss $\ell(y, \widehat{y}) = \mathbb{I}\{\widehat{y} \neq y\}$ is therefore defined by $\ell_{\mathcal{D}}(h) = \mathbb{E}\big[\ell(Y, h(X))\big] = \mathbb{E}\big[\mathbb{I}\{h(X) \neq Y \}\big] =$ 
	$= 1 \cdot \mathbb{P}(h(X) \neq Y) + 0 \cdot \mathbb{P}(h(X) = Y) =  \mathbb{P}(h(X) \neq Y)$<br />
	The Bayes optimal predictor $f^*: \mathcal{X} \to \{−1, 1\}$ for binary classiﬁcation is derived as follows $\quad f^*(x) = \underset{\widehat{y} \in \{-1, 1\}}{\operatorname{argmin}}\mathbb{E}\big[\ell(Y, \widehat{y}) \text{ } \vert \text{ } X = x\big]$
	$= \underset{\widehat{y} \in \{-1, 1\}}{\operatorname{argmin}}\mathbb{E}\Big[\mathbb{I}\big\{Y = +1\big\}\mathbb{I}\big\{\widehat{y} = -1\big\} + \mathbb{I}\big\{Y = -1\big\}\mathbb{I}\big\{\widehat{y} = +1\big\} \text{ } \vert \text{ } X = x\Big]$
	$= \underset{\widehat{y} \in \{-1, 1\}}{\operatorname{argmin}}\Big (  \mathbb{E}\big[\mathbb{I}\{Y = +1\} \vert X = x\big]\mathbb{I}\{\widehat{y} = -1\} +  \mathbb{E}\big[\mathbb{I}\{Y = -1\} \vert X = x\big]\mathbb{I}\{\widehat{y} = +1\}\Big )$
	$= \underset{\widehat{y} \in \{-1, 1\}}{\operatorname{argmin}}\Big(\mathbb{P}\big(Y =  + 1 \text{ } \vert \text{ } X = x\big)\mathbb{I}\{\widehat{y} = -1\} + \mathbb{P}\big(Y = -1 \text{ } \vert \text{ } X = x\big)\mathbb{I}\{\widehat{y} = +1\}\Big)$
	$=\underset{\widehat{y} \in \{-1, 1\}}{\operatorname{argmin}}\Big(\eta(x) \mathbb{I}\{\widehat{y}= -1\} + (1 - \eta(x))\mathbb{I}\{\widehat{y} = +1\}\Big)$<br />
	If $\widehat{y} = 1$, the first term goes to $0$ and we predict the second term. Viceversa, we predict the first term. The algorithm should, then, predict $-1$ when $\eta(x) < (1 - \eta(x))$. So $f^*(x)= \cases{-1 \quad \text{if } \eta(x) < \frac{1}{2} \cr \cr +1 \quad \text{if } \eta(x) \geq \frac{1}{2}}$<br />
	Hence, the Bayes optimal classifier predicts the label whose probability is the highest when conditioned on the instance. Finally, it is easy to verify that the Bayes risk in this case is $\ell_{\mathcal{D}}(f^*) = \mathbb{E}[\ell(f^*(X), Y)] = \mathbb{E}\big[\mathbb{I}\{f^*(X) \neq Y\}\big] = \mathbb{P}(f^*(X) \neq Y)$<br />
	Knowing that $\mathbb{P}(f^*(X) \neq Y \vert X = x) = \operatorname{min} \{\eta(x), (1 - \eta(x))\}$ and knowing that $\mathbb{E}\Big[\mathbb{E}\big[\mathbb{I}\{f^*(x) \neq Y\}\big \vert X = x]\Big] = \mathbb{E}[\mathbb{I}\{f^*(x) \neq Y\}\big]$, it is possible to define $\ell_{\mathcal{D}}(f^*) = \mathbb{E}\big[min\{\eta(X), 1 - \eta(X)\}\big]$.<br /><br />
	That is, the Bayes risk is the expectation of the probability of the event that is less likely to happen conditioned on the instance.
- **Can the Bayes risk for the zero-one loss be zero? If yes, then explain how.**
	
- **Write the formula for Bayes optimal predictor and Bayes risk for the square loss.**
	The Bayes optimal predictor for the square loss is $f^*(x) = \mathbb{E}\big[Y \text{ } \vert \text{ } X = x\big]$ and the Bayes risk for the square loss is $\ell_{\mathcal{D}}(f^*) =  Var\big[Y \vert X = x\big]$.<br />
	We now compute the Bayes optimal predictor $f^*$ for the quadratic loss function $\ell(y, \widehat{y}) = (y − \widehat{y})^2$ when $\mathcal{Y} \equiv \mathbb{R}$, $f^*(x) = \underset{\widehat{y} \in \mathbb{R}}{\operatorname{argmin}} \mathbb{E}\Big[(Y - \widehat{y})^2 \text{ } \vert \text{ } X = x\Big]$<br />
	$= \underset{\widehat{y} \in \mathbb{R}}{\operatorname{argmin}} \mathbb{E}\Big[Y^2 + \widehat{y}^2 -2\widehat{y}Y \text{ } \vert \text{ } X = x\Big]$<br />
	Because we are interested in the minimizing of $\widehat{y}$, it is possible to notice that the term $Y^2$ is a costant and, in fact, it does not depend on $\widehat{y}$. Using the linear property of expectation $\mathbb{E}[aX + bY] = a\mathbb{E}[X] + b\mathbb{E}[Y], \quad a, b \in \mathbb{R}$, we are able to write $= \underset{\widehat{y} \in \mathcal{Y}}{\operatorname {argmin}}\Big(\underbrace{\mathbb{E}\big[Y^2 \text{ }\vert \text{ }X = x \big]}_{\text{does not depend on }\widehat{y}} + \widehat{y}^2 - 2 \widehat{y} \mathbb{E} \big[Y \text{ }\vert \text{ }X = x\big] \Big)$<br />
	$= \underset{\widehat{y} \in \mathbb{R}}{\operatorname {argmin}}\Big( \widehat{y}^2 - 2\widehat{y} \mathbb{E}\big[Y \vert X = x\big]\Big)\quad \text{ } \quad (\text{ignoring the term that does not depend on } \widehat{y})$<br />
	and the reason why this is possible is because the term ignored is just a costant and so it is possible to extract it from the $\operatorname{argmin}$ operator.<br />
	Now, we have to minimize the function $F(\widehat{y}) = \widehat{y}^2 - 2\widehat{y}\mathbb{E}\big[Y \text{ } \vert \text{ } X = x\big]$, which can be seen as $F(z) = z^2 - 2zc$. Its derivative is $F'(z) = 2z -2c$ and it is minimized for $F'(z) = 0$. After the substitution in the derivative, we obtain $2 \widehat{y} - 2c = 2 \widehat{y} - 2 \mathbb{E}\big[Y \vert X = x\big] = 0$ and, therefore 
	$f^*(x) = \mathbb{E}\big[Y \text{ } \vert \text{ } X = x\big]$<br />
	Thus, the Bayes optimal prediction for the quadratic loss function is the expected value of the label conditioned on the instance. So, if it is desired to minimize the loss, the expectation must be predicted.
	Substituting in the conditional risk formula $\mathbb{E}[(Y - f^*(X))^2 \vert X = x]$ the Bayes optimal predictor $f^*(X) = \mathbb{E}[Y \text{ }\vert \text{ } X = x]$ we obtain
	$\mathbb{E}\Big[\big(Y - f^*(X)\big)^2 \text{ }\Big \vert \text{ } X = x \Big] = \mathbb{E}\Big[\big(Y - \mathbb{E}[Y \text{ } \vert \text{ } X = x ]\big)^2 \text{ }\Big \vert \text{ } X = x \Big] = Var\big[Y \vert X = x\big]$<br />
	In words, the conditional risk of the Bayes optimal predictor for the quadratic loss is the **conditional variance**, the variance of the label conditioned on the instance. By averaging over $X$ we obtain $\ell_{\mathcal{D}}(f^*) = \mathbb{E}\big[Var[Y \text{ } \vert \text{ } X]\big]$
- **Explain in mathematical terms the relationship between test error and statistical risk.**
	We can then estimate $\ell_{\mathcal{D}}(h)$ with the **test error**, which is the average loss of $h$ on the test set $\ell_{s'}(h) = \frac{1}{n}\sum_{t = 1}^{n}\ell\big(y_{t}', h(x_t')\big)$.<br />
	Under the assumption that the test set is generated through independent draws from $\mathcal{D}$, the test error corresponds to the **sample mean** of the statistical risk. Indeed, for each $t = 1, ... , n$ the example $(X_t', Y_t')$ is an independent draw from $\mathcal{D}$. Therefore, $\mathbb{E}\Big[\ell\big(Y_t', h(X_t')\big)\Big] = \ell_{\mathcal{D}}(h) \quad \quad t= 1, ..., n$<br />
	Note that the above equalities rely on the assumption that $h$ does not depend on the test set $S'$ (but it depends on the training set). If it did, then the above equalities would not be necessarily true.
- **State the Chernoff-Hoeffding bounds.**
	Let $Z_1 , ... , Z_n$ be **independent and identically distributed random variables** with expectation $\mu$ and such that $0 \leq Z_t \leq 1$ for each $t = 1, ... , n$. Then, for any given $\varepsilon > 0$ <br />
	$\mathbb{P}\Bigg(\frac{1}{n}\sum_{t = 1}^{n}Z_t > \mu + \varepsilon \Bigg) \leq e^{-2\varepsilon^2n}\quad \text{and}\quad \mathbb{P}\Bigg(\frac{1}{n}\sum_{t = 1}^{n}Z_t < \mu - \varepsilon\Bigg) \leq e^{-2\varepsilon^2n}$
- **Write the bias-variance decomposition for a generic learning algorithm $A$ and associate the resulting components to overfitting and underfitting.**
	Fix a finite training set $S$ and let $h_S = A(S)$. The following is called the **bias-variance** decomposition:<br />
	$\ell_{\mathcal{D}}(h_S) = \ell_{\mathcal{D}}(h_S) - \ell_{\mathcal{D}}(h^*)\quad\text{estimation error (large when overfitting)}$<br />
	$\quad + \ell_{\mathcal{D}}(h^*) - \ell_{\mathcal{D}}(f^*)\quad \text{approximation error (large when underfitting)}$<br />
	$\quad + \ell_{\mathcal{D}}(f^*) \quad \text{Bayes error (unavoidable)}$<br />
	where $f^*$ is the Bayes optimal predictor for $(\mathcal{D}, \ell)$. 
- **Write the upper bound on the estimation error of ERM run on a finite class $\mathcal{H}$ of predictors. Bonus points if you justify your answer with a proof.**
	We study the case $\vert \mathcal{H} \vert < \infty$, that is when the model space contains a finite number of predictors. Note that the event $\exists h \in \mathcal{H} : \vert \ell_{\mathcal{D}}(h) - \ell_S(h) \vert > \varepsilon/2$ is the union over $h \in \mathcal{H}$ of the events $\vert \ell_{\mathcal{D}}(h) - \ell_S(h) \vert > \varepsilon/2$. Therefore, by the union bound, we have that its probability is bounded by $\mathcal{H}$ times the probability of the event $\vert \ell_{\mathcal{D}}(h) - \ell_S(h) \vert > \varepsilon/2$ for a single predictor $h \in \mathcal{H}$ is $\leq \vert \mathcal{H} \vert 2 e^{-m\varepsilon^2/2}$. In conclusion, we have that $\mathbb{P}\big(\ell_{\mathcal{D}}(h_S) - \ell_{\mathcal{D}}(h^*) > \varepsilon \big) \leq 2 \vert \mathcal{H} \vert e^{-m \varepsilon^2/2}$. Setting the right-hand side of the equation to $\delta$ and solving for $\varepsilon$, we obtain that $\ell_{\mathcal{D}}(h_S) \leq \ell_{\mathcal{D}}(h^*) + \sqrt{\frac{2}{m}\ln{\frac{2 \vert \mathcal{H} \vert}{\delta}}}$.
- **Write the upper bound on the estimation error of ERM run on the class of complete binary tree predictors with at most $N$ nodes on $d$ binary features.**
	Knowing that $\vert \mathcal{H}_N \vert \leq (2de)^N$ obtained via $\frac{N-1}{2}$-th Catalan number, we can write the upper bound on the estimation error of ERM run on a class of complete binary tree predictors with at most $N$ nodes on $d$ binary features as follows: $\ell_{\mathcal{D}}(h_S) \leq \ell_{\mathcal{D}}(h^*) + \sqrt{\frac{2}{m}\Big(N(1 + \ln{(2d)}) + \ln{\frac{2}{\delta}}\Big)}$.<br />
	From that. we deduce that on this case a training set of size of order $N \ln{d}$ is enough to control the risk of $h_s \in \mathcal{H}_N$.
- **Write the bound on the difference between risk and training error for an arbitrary complete binary tree classifier $h$ on $d$ binary features in terms of its number $N_h$ of nodes. Bonus points if you provide a short explanation on how this bound is obtained.**
	$\ell_{\mathcal{D}}(h) \leq \ell_S(h) + \sqrt{\frac{1}{2m}\Big(\vert \sigma(h) \vert + \ln{\frac{2}{\delta}}\Big)}$<br />
	Hence, with Occam Razor given two predictor with the same training error the simpler (the shortest $\vert \sigma \vert$) is the best.<br />
	We upper bound the risk of a tree predictor $h$ by its training error plus a quantity $\varepsilon_h$ that now depends on the size of the tree.<br />
	We introduce a function $w: \mathcal{H} \to [0, 1]$ and call $w(h)$ the weight of tree predictor $h$. We assume $\sum_{h \in \mathcal{H}} w(h) \leq 1$.<br />
	And now choosing $\varepsilon_h = \sqrt{\frac{1}{2m}\Big(\ln{\frac{1}{w(h)}} + \ln{\frac{2}{\delta}}\Big)}$, we get that $\mathbb{P}(\exists h \in \mathcal{H}) : \vert \ell_{\mathcal{D}}(h) - \ell_S(h) \vert > \varepsilon_h \geq \sum_{h \in \mathcal{H}} \delta w(h) \leq \delta$.<br />
	A consequence of this analysis is that, with probability at least $1 - \delta$ with respect to the training set random draw, we have that $\ell_{\mathcal{D}}(h) \leq \ell_S(h) + \sqrt{\frac{1}{2m}\Big(\ln{\frac{1}{w(h)}} + \ln{\frac{2}{\delta}}\Big)}$.<br />
	Now, using a theoretic technique we can encode each tree predictor $h$ as a binary string $\sigma(h)$ of length $\vert \sigma(h) \vert = \mathcal{O}(N_h \log{d})$. Thanks to Kraft inequality, we can associate a weight $w(h) = 2^{-\vert \sigma(h) \vert}$ to each tree predictor $h$ in order to get the bound shown at the beginning.
- **Write the formula for the $K$-fold cross validation estimate. Explain the main quantities occurring in the formula.**
	The **$K$-fold CV estimate** of $\mathbb{E}\big[\ell_{\mathcal{D}}(A)\big]$ on $S$, denoted by $\ell_S^{CV}(A)$, is computed as follows: we run $A$ on each training part $S_{-i}$ of the folds $i = 1, ..., K$ and obtain the predictors $h_1 =A(S_{-1}), ..., h_K =  A(S_{-K})$. We then compute the (rescaled) errors on the testing part of each fold, $\ell_{S_i}(h_i) = \frac{K}{m} \sum_{(x, y) \in  S_i} \ell(y, h_i(x))$.<br />
	Finally, we compute the CV estimate by averaging these errors $\ell_{S}^{CV}(A) = \frac{1}{K} \sum_{i = 1}^{K} \ell_{S_i}(h_i)$.
- **Write the pseudo-code for computing the nested cross validation estimate.**
	![[K-fold Nested Cross Validation.png]]
- **Write the mathematical definition of consistency for an algorithm $A$.**
	A learning algorithm $A$ is **consistent** with respect to a loss function $\ell$ if for any data distribution $\mathcal{D}$ it holds that $\underset{m \to \infty}{\operatorname{lim}}\mathbb{E}\Big[\ell_{\mathcal{D}}(A(S_m))\Big] = \ell_{\mathcal{D}}(f^*)$ where the expectation is with respect to the random draw of the training set $S_m$ of size $m$ from the distribution $\mathcal{D}$, and $\ell_{\mathcal{D}}(f^*)$ is the Bayes risk for $(\mathcal{D}, \ell)$.
- **Write the statement of the no-free-lunch theorem.**
	For any sequence $a_1 , a_2 , . . .$ of positive numbers converging to zero and such that $\frac{1}{16}≥ a_1 ≥ a_2 ≥ · · ·$ and for any consistent learning algorithm $A$ for binary classification with zero-one loss, there exists a data distribution $\mathcal{D}$ such that $\ell_{\mathcal{D}}(f^*) = 0$ and $\mathbb{E}\Big[\ell_{\mathcal{D}}(A(S_m))\Big] \geq a_m \quad \forall m \geq 1$.
- **Write the mathematical definition of nonparametric learning algorithm. Define the main quantities occurring in the formula.**
	We say that $A$ is a **nonparametric learning algorithm** if $A$'s approssimation error vanishes as $m$ grows to infinity. Formally, $\underset{m \to \infty}{\operatorname{lim}}\underset{h \in \mathcal{H}_m}{\operatorname{min}}\ell_{\mathcal{D}}(h^*) = \ell_{\mathcal{D}}(f^*)$.
- **Name one nonparametric learning algorithm and one parametric learning algorithm.**
	Some examples of nonparametric learning algorithms are $k$-nearest neighboors and greedy decision tree classifiers. Some examples of parametric learning algorithms are linear regression and logistic regression.
- **Write the mathematical conditions on $k$ ensuring consistency for the $k-NN$ algorithm.**
	The standard $k-NN$ algorithm is nonparametric but not known to be consistent for any fixed value of $k$. Indeed, one can only show that $\underset{m \to \infty}{\operatorname{lim}}\mathbb{E}\Big[\ell_{\mathcal{D}}(k-NN(S_m))\Big] \leq \ell_{\mathcal{D}}(f^*) + 2 \sqrt{\frac{\ell_{\mathcal{D}}(f^*)}{k}}$ for any data distribution $\mathcal{D}$. However, if we let $k$ be chosen as a function $k_m$ of the training set size, then the algorithm becomes consistent provided $k_m \to \infty$ and $k_m = o(m)$.
- **Write the formula for the Lipschitz condition in a binary classification problem. Define the main quantities occurring in the formula.**
	We may define consistency with respect to a restricted class of distributions $\mathcal{D}$. For example, in binary classification we may restrict to all distributions $\mathcal{D}$ such that $\eta(x) = \mathbb{P}(Y = 1 \vert X = x)$ is a Lipschitz function on $\mathcal{X}$. Formally, there exists $0 < c < \infty$ such that $\vert \eta(x) - \eta(x')\vert \leq c \Vert x - x' \Vert \quad \text{ for all } x, x' \in \mathcal{X}$. Technically, this conditions implies that $\eta$ is Lipschitz in $\mathcal{X}$. This is a restriction on the set of all allowed $\eta$ as $c < \infty$ implies continuity (but the opposite is not true).
- **Write the rate at which the risk of a consistent learning algorithm for binary classification vanish as a function of the training set size $m$ and the dimension $d$ under Lipschitz assumptions.**
	Under Lipschitz assumptions on $\eta$, the typical convergence rate to Bayes risk is $m^{−1/(d+1)}$ (exponentially slow in $d$).
- **Explain the curse of dimensionality.**
	Te convergence rate $m^{\frac{-1}{d+1}}$ implies that to get $\varepsilon$-close to the Bayes Risk, we need a training set size $m$ of order $\varepsilon^{-(d+1)}$. This exponential dependence on the number of features of the training set size is known a curse of dimensionality and refers to the difficulty of learning in a nonparametric setting when datapoints live in a high-dimensional space.
- **Why non parametric algorithm cannot be consistent?**
- **Write the bound on the risk of $1-NN$ binary classifier under Lipschitz assumptions.**
- **Can the ERM over linear classifiers be computed efficiently? Can it be approximated efficiently? Motivate your answers.**
- **Write the system of linear inequalities stating the condition of linear separability for a training set for binary classfication.**
- **Write the pseudo-code for the Perceptron algorithm.**
- **Write the statement of the Perceptron convergence theorem. Bonus points if you provide the proof.**
- **Write the closed-form formula (i.e., not the argmin definition) for the Ridge Regression predictor. Define the main quantities occurring in the formula.**
- **Write the pseudo-code for the projected online gradient descent algorithm.**
- **Write the upper bound on the regret of projected online gradient descent on convex functions. Define the main quantities occurring in the bound.**
- **Write the upper bound on the regret of online gradient descent on $\sigma$-strongly convex functions. Define the main quantities occurring in the bound.**
- **Write the formula for the hinge loss.**
- **Write the mistake bound for the Perceptron run on an arbitrary data stream for binary classification. Define the main quantities occurring in the bound.**
- **Write the formula for the polynomial kernel of degree $n$.**
- **Write the formula for the Gaussian kernel with parameter $\gamma$.**
- **Write the pseudo-code for the kernel Perceptron algorithm.**
- **Write the mathematical definition of the linear space $\mathcal{H}_K$ of functions induced by a kernel $K$.**
- **Let $f$ be an element of the linear space $\mathcal{H}_K$ induced by a kernel $K$. Write $f(x)$ in terms of $K$.**
- **Write the statement of the Perceptron convergence theorem when the Perceptron is run with a kernel $K$. Define the main quantities occurring in the bound. (BONUS)**
- **Write the mistake bound of the Perceptron convergence theorem when the Perceptron is run with a kernel $K$. Define the main quantities occurring in the bound.**
- **Write the mistake bound for the kernel Perceptron run on an arbitrary data stream for binary classification. Define the main quantities occurring in the bound.**
- **Write the closed-form formula (i.e., not the argmin definition) of the kernel version of the Ridge Regression predictor.**
- **Write the convex optimization problem with linear constraints that defines the SVM hyperplane in the linearly separable case.**
- **Write the unconstrained optimization problem whose solution defines the SVM hyperplane when the training set is not necessarily linearly separable.**
- **Write the bound on the expected value of the SVM objective function achieved by Pegasos. Provide also a bound on the expected squared norm of the loss gradient.**
- **Write the definition of $\varepsilon$-stability for a learning algorithm.**
- **Write the value of $\varepsilon$ for which SVM is known to be stable. The value depends on the radius $X$ of the ball where the training datapoints live, the training set size $m$, and the regularization coefficient $\lambda$.**
- **Write the mathematical conditions on the regularization coefficient $\lambda$ ensuring consistency for the SVM algorithm wih Gaussian kernel.**
- **Consider the class $\mathcal{F}_d$ of all functions of the form $f : \{−1, 1\}^d \to \{−1, 1\}$. Let $\mathcal{F}_{G,\operatorname{sgn}}$ be the class of functions computed by a feedforward neural networks with the $\operatorname{sgn}$ activation function and graph $G = (V, E)$. Provide asymptotic upper and lower bounds on $\vert V \vert$ such that $\mathcal{F}_d \subseteq \mathcal{F}_{G,\operatorname {sgn}}$.**
- **Define a class of neural networks for which the ERM problem with the square loss is probably NP-hard.**
- **Write the update line of the stochastic gradient descent algorithm. Explain the main quantities.**
- **Write the definition of logistic loss for logistic regression with linear models.**
- **Write the definition of consistency for surrogate losses.**
- **Write a sufficient condition for consistency of a surrogate loss.**
- **Write the formula for Bayes optimal predictor and Bayes risk for the logistic loss.**